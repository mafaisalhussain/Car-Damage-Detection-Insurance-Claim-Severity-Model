{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH90gGynZgSd",
        "outputId": "09505898-be60-460d-97fe-3aff6b67b79b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark\n",
        "!pip install pyspark\n",
        "\n",
        "import os\n",
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "if 'spark' not in globals():\n",
        "    print(\"Spark session not found, creating a new one...\")\n",
        "    findspark.init()\n",
        "    spark = (\n",
        "        SparkSession.builder\n",
        "        .appName(\"CarInsuranceClaimEstimator\")\n",
        "        .config(\"spark.driver.memory\", \"4g\")\n",
        "        .config(\"spark.executor.memory\", \"4g\")\n",
        "        # Removed the HDFS configuration, as we intend to save locally.\n",
        "        #.config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\")\n",
        "        .getOrCreate()\n",
        "    )\n",
        "    print(\"Spark session created.\")\n",
        "else:\n",
        "    print(\"Spark session already exists.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jexnwSG0NFh",
        "outputId": "f6e748b0-2f96-4c38-fb3c-6f0ebbe0d84a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.12/dist-packages (2.0.1)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Spark session already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import PipelineModel\n",
        "\n",
        "MODEL_PATH = \"/content/drive/MyDrive/car_insurance_gbt_model_saved\"\n",
        "\n",
        "gbt_live_model = PipelineModel.load(MODEL_PATH)\n",
        "print(\"Model loaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibkaqL5nZ5-p",
        "outputId": "68e72621-4659-4db5-fd47-d396dcc927aa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "KNN_PATH = \"/content/drive/MyDrive/damage_type_knn.pkl\"\n",
        "knn_clf = joblib.load(KNN_PATH)\n",
        "\n",
        "print(\"KNN classifier loaded!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHGMWz5dcdQN",
        "outputId": "0e1c17a8-8b88-4272-c35a-e8b21086590c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN classifier loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import joblib\n",
        "\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.sql.types import (\n",
        "    StructType, StructField, StringType,\n",
        "    IntegerType, FloatType, ArrayType\n",
        ")\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "\n",
        "from tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "\n",
        "# GBT_PATH = \"/content/drive/MyDrive/car_insurance_gbt_model_saved\"\n",
        "KNN_PATH = \"/content/drive/MyDrive/damage_type_knn.pkl\"\n",
        "\n",
        "print(\"Loading GBT regression model...\")\n",
        "gbt_live_model = PipelineModel.load(GBT_PATH)\n",
        "print(\"âœ” Loaded GBT model\")\n",
        "\n",
        "print(\"Loading KNN damage-type classifier...\")\n",
        "knn_clf = joblib.load(KNN_PATH)\n",
        "print(\"âœ” Loaded KNN model\")\n",
        "\n",
        "print(\"Loading EfficientNet feature extractor...\")\n",
        "cnn_live = EfficientNetB0(weights=\"imagenet\", include_top=False, pooling=\"avg\")\n",
        "print(\"âœ” Loaded CNN model\")\n",
        "\n",
        "\n",
        "\n",
        "# 2. Convert uploaded image -> EfficientNet feature vector\n",
        "\n",
        "def pil_to_features(pil_img):\n",
        "    pil_img = pil_img.convert(\"RGB\").resize((224, 224))\n",
        "    x = np.array(pil_img).astype(\"float32\")\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    features = cnn_live.predict(x, verbose=0)[0]\n",
        "    return features.astype(\"float32\")\n",
        "\n",
        "\n",
        "\n",
        "# 3. MAIN PREDICTION FUNCTION\n",
        "\n",
        "def predict_claim(\n",
        "    img,\n",
        "    driver_age,\n",
        "    car_year,\n",
        "    mileage,\n",
        "    auto_damage_type,\n",
        "    manual_damage_type\n",
        "):\n",
        "    if img is None:\n",
        "        return \"âš  Please upload a car damage image.\"\n",
        "\n",
        "    # A. Extract image features\n",
        "    features = pil_to_features(img)\n",
        "\n",
        "    # B. Compute severity using vector math\n",
        "    vector_magnitude = float(np.linalg.norm(features))\n",
        "    vector_variance  = float(np.var(features))\n",
        "\n",
        "    damage_index = 0.6 * vector_magnitude + 50.0 * vector_variance\n",
        "    severity_raw = np.log1p(damage_index) / 2.0\n",
        "    severity_score = float(np.clip(severity_raw, 0.5, 5.0))\n",
        "\n",
        "    # C. Determine damage type\n",
        "    if auto_damage_type:\n",
        "        damage_type = knn_clf.predict(features.reshape(1, -1))[0]\n",
        "        source = \"Auto (KNN)\"\n",
        "    else:\n",
        "        damage_type = manual_damage_type\n",
        "        source = \"Manual selection\"\n",
        "\n",
        "    # D. Spark dataframe for GBT\n",
        "    schema = StructType([\n",
        "        StructField(\"claim_id\",       StringType(), True),\n",
        "        StructField(\"image_path\",     StringType(), True),\n",
        "        StructField(\"features\",       ArrayType(FloatType()), True),\n",
        "        StructField(\"driver_age\",     IntegerType(), True),\n",
        "        StructField(\"car_year\",       IntegerType(), True),\n",
        "        StructField(\"mileage\",        IntegerType(), True),\n",
        "        StructField(\"damage_type\",    StringType(), True),\n",
        "        StructField(\"severity_score\", FloatType(), True),\n",
        "    ])\n",
        "\n",
        "    row = [(\n",
        "        \"ui_claim_1\",\n",
        "        \"uploaded_image\",\n",
        "        features.tolist(),\n",
        "        int(driver_age),\n",
        "        int(car_year),\n",
        "        int(mileage),\n",
        "        str(damage_type),\n",
        "        float(severity_score),\n",
        "    )]\n",
        "\n",
        "    sdf = spark.createDataFrame(row, schema)\n",
        "    to_vec = udf(lambda xs: Vectors.dense(xs), VectorUDT())\n",
        "    sdf = sdf.withColumn(\"features_vec\", to_vec(\"features\"))\n",
        "\n",
        "    # E. Price prediction\n",
        "    pred_df = gbt_live_model.transform(sdf)\n",
        "    result = pred_df.select(\"damage_type\", \"severity_score\", \"prediction\").collect()[0]\n",
        "    cost = float(result[\"prediction\"])\n",
        "\n",
        "    # F. Format output\n",
        "    output = []\n",
        "    output.append(\"###  Image Vector Analysis\")\n",
        "    output.append(f\"- Magnitude: **{vector_magnitude:.4f}**\")\n",
        "    output.append(f\"- Variance: **{vector_variance:.6f}**\")\n",
        "    output.append(f\"- Damage Index: **{damage_index:.4f}**\\n\")\n",
        "\n",
        "    output.append(\"###  AI Diagnosis\")\n",
        "    output.append(f\"- Damage Type: **{damage_type}**  _(Source: {source})_\")\n",
        "    output.append(f\"- Severity Score: **{severity_score:.2f} / 5.0**\\n\")\n",
        "\n",
        "    output.append(\"###  Vehicle Info\")\n",
        "    output.append(f\"- Driver Age: **{driver_age}**\")\n",
        "    output.append(f\"- Car Year: **{car_year}**\")\n",
        "    output.append(f\"- Mileage: **{int(mileage):,} miles**\\n\")\n",
        "\n",
        "    output.append(\"###  Estimated Repair Cost\")\n",
        "    output.append(f\"- **${cost:,.2f}**\")\n",
        "\n",
        "    if cost > 5000:\n",
        "        output.append(\"âž¡ **Conclusion: MAJOR DAMAGE / POSSIBLE TOTAL LOSS**\")\n",
        "    elif cost > 2500:\n",
        "        output.append(\"âž¡ **Conclusion: Significant Repair Required**\")\n",
        "    else:\n",
        "        output.append(\"âž¡ **Conclusion: Minor to Moderate Repair**\")\n",
        "\n",
        "    return \"\\n\".join(output)\n",
        "\n",
        "\n",
        "\n",
        "# 4. BUILD GRADIO INTERFACE\n",
        "\n",
        "damage_options = [\"scratch\", \"crack\", \"dent\", \"tire_flat\", \"glass_shatter\"]\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=predict_claim,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"pil\", label=\"Upload a damaged car image\"),\n",
        "        gr.Slider(18, 80, value=35, step=1, label=\"Driver Age\"),\n",
        "        gr.Slider(2000, 2024, value=2015, step=1, label=\"Car Year\"),\n",
        "        gr.Number(value=60000, label=\"Mileage\"),\n",
        "        gr.Checkbox(value=True, label=\"Auto-detect damage type\"),\n",
        "        gr.Dropdown(damage_options, value=\"scratch\",\n",
        "                    label=\"Manual damage type (if auto OFF)\"),\n",
        "    ],\n",
        "    outputs=gr.Markdown(label=\"AI Assessment\"),\n",
        "    title=\"ðŸš— AI Car Damage Claim Estimator\",\n",
        "    description=\"Upload a photo and enter car details to estimate repair cost.\",\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "xIlq_oi-cfwk",
        "outputId": "e7640bea-3ed5-49a6-bff0-3bc5c7d0df8e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GBT regression model...\n",
            "âœ” Loaded GBT model\n",
            "Loading KNN damage-type classifier...\n",
            "âœ” Loaded KNN model\n",
            "Loading EfficientNet feature extractor...\n",
            "âœ” Loaded CNN model\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5160666987a596bff8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5160666987a596bff8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TJLlgT04d6Hp"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}